---
title: "hw3"
author: "Gunnar"
date: "10/19/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Load libraries
```{r}
library(tidyverse)
library(janitor)
```

**Problem 1**

Loading dataset
```{r}
library(p8105.datasets)
data("instacart")
```
Exploring instacart
```{r}
names(instacart)
summary(instacart)

instacart %>%
  distinct(user_id) %>%
  nrow()
  
instacart %>%
  group_by(department) %>%
  summarise(mean(order_hour_of_day, na.rm = TRUE))

instacart %>%
  count(aisle, sort = TRUE)
```

The instacart dataset has 1,384,617 observations over 15 variables. Key variables in the data set include product name, department, order hour of day, and reordered. For example, there were 131,209 distinct user ids and the mean order hour of the day for products from the alcohol department was 13.7. 

There were 134 aisles and the aisles with the most orders were fresh vegetables with 150,609 orders, fresh fruits with 150,473 orders, and packaged vegetables and fruits with 78,493 orders. 

Maing a plot of orders by aisle
```{r}
instacart %>%
  filter(aisle >= 10000) %>%
  ggplot(aes(y = forcats::fct_infreq(aisle))) +
  geom_bar() +
  labs(y = "Aisle", x = "Number of Orders") +
  theme(axis.text.y = element_text(size = 3))
```

Making a table of top three items for baking ingredients, dog food care, and packaged fruits and vegetables
Steps:
1. filter to relevant aisles
2. group by aisle and identify top 3 rows
3. Add ranking
4. Pivot the table wider and unite products with purchase counts
```{r}
top3_table <- instacart %>%
  filter(aisle == "dog food care" | aisle == "baking ingredients" | aisle == "packaged vegetables fruits") %>%
  select(aisle, product_name) %>%
  group_by(aisle) %>%
  count(product_name, sort = TRUE) %>%
  slice_max(order_by = n, n = 3) %>%
  mutate(rank = rep(1:3, length.out = 3)) %>%
  pivot_wider(names_from = aisle, values_from = c(product_name, n)) %>%
  unite("baking ingredients", 5, 2, sep = " orders of ", remove = TRUE) %>%
  unite("dog food care", 5, 3, sep = " orders of ", remove = TRUE)  %>%
  unite("packaged vegetables and fruits", 5, 4, sep = " orders of ") %>%
  knitr::kable(caption = "Three Most Popular Items by Aisle"
  )
top3_table
```

Making table of mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

Steps:
1. Filter to relevant products
2. Change days from numbers to strings
3. group by day and product and find mean time
4. pivot wider
```{r}
hour_table <- instacart %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  mutate(order_dow = factor(order_dow, levels = 0:6, labels = c("sunday", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday"))) %>%
  select(product_name, order_dow, order_hour_of_day) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_time = mean(order_hour_of_day, na.rm = TRUE)) %>%
  pivot_wider(names_from = order_dow, values_from = mean_time) %>%
  knitr::kable(caption = "Ice Cream and Apple Purchases by Day", digits = 2)
hour_table
```

**Problem 2**

Loading BRFSS data
```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

Cleaning the data set
```{r}
brfss <- brfss_smart2010 %>%
  clean_names() %>%
  filter(topic == "Overall Health", response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>%
  rename(state = locationabbr) %>%
  rename(location = locationdesc) %>%
  mutate(response = factor(response, labels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

Part 2 - answering questions
States with 7+ observations in 2010 and 2002
```{r}
brfss %>%
  filter(year == 2002) %>%
  group_by(state) %>%
  summarise(n_locations = n_distinct(location)) %>%
  filter(n_locations >= 7)
  
brfss %>%
  filter(year == 2010) %>%
  group_by(state) %>%
  summarise(n_locations = n_distinct(location)) %>%
  filter(n_locations >= 7)
```

In 2002, the states with 7 or more locations in the BRFSS data were CT, FL, MA, NC, NC, NJ, and PA.

In 2010, the states with 7 or more locations in the BRFSS data were CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA. 

Spaghetti plot of excellent response by state over time
```{r}
spag_plot <- brfss %>%
  filter(response == "Excellent") %>%
  group_by(state, year) %>%
  summarise(avg_data = mean(data_value, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = avg_data)) +
  geom_line(aes(color = state)) +
  xlab("Year") +
  ylab("Average Data Value") +
  ggtitle("Spaghetti Plot of Average Data Values by State between 2002 and 2010")
```
In the spaghetti plot of average data values by state between 2002 and 2010, many states fluctuate widely from year to year and it is difficult to discern which states are increasing or decresing in average data value over time as the 50 lines overlap.


Facet plot of poor to excellent responses in NY in 2002 and 2010
```{r}
facet_plot <- brfss %>%
  filter(state == "NY", year == 2002 | year == 2010) %>%
  group_by(state, year) %>%
  ggplot(aes(x = data_value)) +
  geom_histogram() +
  ylab("Count") +
  xlab("Data Value") +
  ggtitle("Distribution of New York Data Values in 2002 and 2010") +
  facet_grid(cols = vars(year))
```

From the facet grid of New York data values in 2002 and 2010, data values were distributed around 27 with a left skew in 2002. In 2010, data values were less normally distributed as it appears there were an equal number of 0 values as 30 values. 
