hw3
================
Gunnar
10/19/2021

Load libraries

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──

    ## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4
    ## ✓ tibble  3.1.4     ✓ dplyr   1.0.7
    ## ✓ tidyr   1.1.3     ✓ stringr 1.4.0
    ## ✓ readr   2.0.1     ✓ forcats 0.5.1

    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(janitor)
```

    ## 
    ## Attaching package: 'janitor'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     chisq.test, fisher.test

**Problem 1**

Loading dataset

``` r
library(p8105.datasets)
data("instacart")
```

Exploring instacart

``` r
names(instacart)
```

    ##  [1] "order_id"               "product_id"             "add_to_cart_order"     
    ##  [4] "reordered"              "user_id"                "eval_set"              
    ##  [7] "order_number"           "order_dow"              "order_hour_of_day"     
    ## [10] "days_since_prior_order" "product_name"           "aisle_id"              
    ## [13] "department_id"          "aisle"                  "department"

``` r
summary(instacart)
```

    ##     order_id         product_id    add_to_cart_order   reordered     
    ##  Min.   :      1   Min.   :    1   Min.   : 1.000    Min.   :0.0000  
    ##  1st Qu.: 843370   1st Qu.:13380   1st Qu.: 3.000    1st Qu.:0.0000  
    ##  Median :1701880   Median :25298   Median : 7.000    Median :1.0000  
    ##  Mean   :1706298   Mean   :25556   Mean   : 8.758    Mean   :0.5986  
    ##  3rd Qu.:2568023   3rd Qu.:37940   3rd Qu.:12.000    3rd Qu.:1.0000  
    ##  Max.   :3421070   Max.   :49688   Max.   :80.000    Max.   :1.0000  
    ##     user_id         eval_set          order_number      order_dow    
    ##  Min.   :     1   Length:1384617     Min.   :  4.00   Min.   :0.000  
    ##  1st Qu.: 51732   Class :character   1st Qu.:  6.00   1st Qu.:1.000  
    ##  Median :102933   Mode  :character   Median : 11.00   Median :3.000  
    ##  Mean   :103113                      Mean   : 17.09   Mean   :2.701  
    ##  3rd Qu.:154959                      3rd Qu.: 21.00   3rd Qu.:5.000  
    ##  Max.   :206209                      Max.   :100.00   Max.   :6.000  
    ##  order_hour_of_day days_since_prior_order product_name          aisle_id    
    ##  Min.   : 0.00     Min.   : 0.00          Length:1384617     Min.   :  1.0  
    ##  1st Qu.:10.00     1st Qu.: 7.00          Class :character   1st Qu.: 31.0  
    ##  Median :14.00     Median :15.00          Mode  :character   Median : 83.0  
    ##  Mean   :13.58     Mean   :17.07                             Mean   : 71.3  
    ##  3rd Qu.:17.00     3rd Qu.:30.00                             3rd Qu.:107.0  
    ##  Max.   :23.00     Max.   :30.00                             Max.   :134.0  
    ##  department_id      aisle            department       
    ##  Min.   : 1.00   Length:1384617     Length:1384617    
    ##  1st Qu.: 4.00   Class :character   Class :character  
    ##  Median : 8.00   Mode  :character   Mode  :character  
    ##  Mean   : 9.84                                        
    ##  3rd Qu.:16.00                                        
    ##  Max.   :21.00

``` r
instacart %>%
  distinct(user_id) %>%
  nrow()
```

    ## [1] 131209

``` r
instacart %>%
  group_by(department) %>%
  summarise(mean(order_hour_of_day, na.rm = TRUE))
```

    ## # A tibble: 21 × 2
    ##    department      `mean(order_hour_of_day, na.rm = TRUE)`
    ##    <chr>                                             <dbl>
    ##  1 alcohol                                            13.7
    ##  2 babies                                             13.5
    ##  3 bakery                                             13.5
    ##  4 beverages                                          13.5
    ##  5 breakfast                                          13.5
    ##  6 bulk                                               13.6
    ##  7 canned goods                                       13.6
    ##  8 dairy eggs                                         13.6
    ##  9 deli                                               13.6
    ## 10 dry goods pasta                                    13.7
    ## # … with 11 more rows

``` r
instacart %>%
  count(aisle, sort = TRUE)
```

    ## # A tibble: 134 × 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

The instacart dataset has 1,384,617 observations over 15 variables. Key
variables in the data set include product name, department, order hour
of day, and reordered. For example, there were 131,209 distinct user ids
and the mean order hour of the day for products from the alcohol
department was 13.7.

There were 134 aisles and the aisles with the most orders were fresh
vegetables with 150,609 orders, fresh fruits with 150,473 orders, and
packaged vegetables and fruits with 78,493 orders.

Maing a plot of orders by aisle

``` r
instacart %>%
  filter(aisle >= 10000) %>%
  ggplot(aes(y = forcats::fct_infreq(aisle))) +
  geom_bar() +
  labs(y = "Aisle", x = "Number of Orders") +
  theme(axis.text.y = element_text(size = 3))
```

![](p8105_hw3_jgc2157_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

Making a table of top three items for baking ingredients, dog food care,
and packaged fruits and vegetables Steps: 1. filter to relevant aisles
2. group by aisle and identify top 3 rows 3. Add ranking 4. Pivot the
table wider and unite products with purchase counts

``` r
top3_table <- instacart %>%
  filter(aisle == "dog food care" | aisle == "baking ingredients" | aisle == "packaged vegetables fruits") %>%
  select(aisle, product_name) %>%
  group_by(aisle) %>%
  count(product_name, sort = TRUE) %>%
  slice_max(order_by = n, n = 3) %>%
  mutate(rank = rep(1:3, length.out = 3)) %>%
  pivot_wider(names_from = aisle, values_from = c(product_name, n)) %>%
  unite("baking ingredients", 5, 2, sep = " orders of ", remove = TRUE) %>%
  unite("dog food care", 5, 3, sep = " orders of ", remove = TRUE)  %>%
  unite("packaged vegetables and fruits", 5, 4, sep = " orders of ") %>%
  knitr::kable(caption = "Three Most Popular Items by Aisle"
  )
top3_table
```

| rank | baking ingredients              | dog food care                                              | packaged vegetables and fruits      |
|-----:|:--------------------------------|:-----------------------------------------------------------|:------------------------------------|
|    1 | 499 orders of Light Brown Sugar | 30 orders of Snack Sticks Chicken & Rice Recipe Dog Treats | 9784 orders of Organic Baby Spinach |
|    2 | 387 orders of Pure Baking Soda  | 28 orders of Organix Chicken & Brown Rice Recipe           | 5546 orders of Organic Raspberries  |
|    3 | 336 orders of Cane Sugar        | 26 orders of Small Dog Biscuits                            | 4966 orders of Organic Blueberries  |

Three Most Popular Items by Aisle

Making table of mean hour of the day at which Pink Lady Apples and
Coffee Ice Cream are ordered on each day of the week

Steps: 1. Filter to relevant products 2. Change days from numbers to
strings 3. group by day and product and find mean time 4. pivot wider

``` r
hour_table <- instacart %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  mutate(order_dow = factor(order_dow, levels = 0:6, labels = c("sunday", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday"))) %>%
  select(product_name, order_dow, order_hour_of_day) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_time = mean(order_hour_of_day, na.rm = TRUE)) %>%
  pivot_wider(names_from = order_dow, values_from = mean_time) %>%
  knitr::kable(caption = "Three Most Popular Items by Aisle", digits = 2)
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the `.groups` argument.

``` r
hour_table
```

| product\_name    | sunday | monday | tuesday | wednesday | thursday | friday | saturday |
|:-----------------|-------:|-------:|--------:|----------:|---------:|-------:|---------:|
| Coffee Ice Cream |  13.77 |  14.32 |   15.38 |     15.32 |    15.22 |  12.26 |    13.83 |
| Pink Lady Apples |  13.44 |  11.36 |   11.70 |     14.25 |    11.55 |  12.78 |    11.94 |

Three Most Popular Items by Aisle

**Problem 2**

Loading BRFSS data

``` r
library(p8105.datasets)
data("brfss_smart2010")
```

Cleaning the data set

``` r
brfss <- brfss_smart2010 %>%
  clean_names() %>%
  filter(topic == "Overall Health", response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>%
  rename(state = locationabbr) %>%
  rename(location = locationdesc) %>%
  mutate(response = factor(response, labels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

Part 2 - answering questions States with 7+ observations in 2010 and
2002

``` r
brfss %>%
  filter(year == 2002) %>%
  group_by(state) %>%
  summarise(n_locations = n_distinct(location)) %>%
  filter(n_locations >= 7)
```

    ## # A tibble: 6 × 2
    ##   state n_locations
    ##   <chr>       <int>
    ## 1 CT              7
    ## 2 FL              7
    ## 3 MA              8
    ## 4 NC              7
    ## 5 NJ              8
    ## 6 PA             10

``` r
brfss %>%
  filter(year == 2010) %>%
  group_by(state) %>%
  summarise(n_locations = n_distinct(location)) %>%
  filter(n_locations >= 7)
```

    ## # A tibble: 14 × 2
    ##    state n_locations
    ##    <chr>       <int>
    ##  1 CA             12
    ##  2 CO              7
    ##  3 FL             41
    ##  4 MA              9
    ##  5 MD             12
    ##  6 NC             12
    ##  7 NE             10
    ##  8 NJ             19
    ##  9 NY              9
    ## 10 OH              8
    ## 11 PA              7
    ## 12 SC              7
    ## 13 TX             16
    ## 14 WA             10

In 2002, the states with 7 or more locations in the BRFSS data were CT,
FL, MA, NC, NC, NJ, and PA.

In 2010, the states with 7 or more locations in the BRFSS data were CA,
CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA.

Spaghetti plot of excellent response by state over time

``` r
spag_plot <- brfss %>%
  filter(response == "Excellent") %>%
  group_by(state, year) %>%
  summarise(avg_data = mean(data_value, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = avg_data)) +
  geom_line(aes(color = state)) +
  xlab("Year") +
  ylab("Average Data Value") +
  ggtitle("Spaghetti Plot of Average Data Values by State between 2002 and 2010")
```

    ## `summarise()` has grouped output by 'state'. You can override using the `.groups` argument.

In the spaghetti plot of average data values by state between 2002 and
2010, many states fluctuate widely from year to year and it is difficult
to discern which states are increasing or decresing in average data
value over time as the 50 lines overlap.

Facet plot of poor to excellent responses in NY in 2002 and 2010

``` r
facet_plot <- brfss %>%
  filter(state == "NY", year == 2002 | year == 2010) %>%
  group_by(state, year) %>%
  ggplot(aes(x = data_value)) +
  geom_histogram() +
  ylab("Count") +
  xlab("Data Value") +
  ggtitle("Distribution of New York Data Values in 2002 and 2010") +
  facet_grid(cols = vars(year))
```

From the facet grid of New York data values in 2002 and 2010, data
values were distributed around 27 with a left skew in 2002. In 2010,
data values were less normally distributed as it appears there were an
equal number of 0 values as 30 values.
